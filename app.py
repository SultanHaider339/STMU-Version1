# critical_thinking_analyzer.py
# Analyzes text using Paul's Standards of Critical Thinking

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import io
import json
import torch
import pypdf
import docx
import re
from datetime import datetime
from typing import List, Dict, Any, Union
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import numpy as np

# ============================================================
#       PAUL'S STANDARDS OF CRITICAL THINKING - DEFINITIONS
# ============================================================

PAUL_STANDARDS = {
    "clarity": {
        "name": "Clarity",
        "color": "#3498DB",
        "icon": "ğŸ”",
        "description": "Is the statement clear and understandable?",
        "question": "Could you elaborate? Could you illustrate? Could you give an example?",
        "indicators_positive": ["specifically", "for example", "in other words", "to illustrate", "meaning", "that is", "namely", "such as", "defined as", "to clarify"],
        "indicators_negative": ["somehow", "something", "stuff", "things", "whatever", "kind of", "sort of", "like", "you know", "etc"],
        "weight": 1.0
    },
    "accuracy": {
        "name": "Accuracy",
        "color": "#2ECC71",
        "icon": "âœ“",
        "description": "Is the statement true and free from errors?",
        "question": "How could we verify this? How could we find out if this is true?",
        "indicators_positive": ["according to", "research shows", "data indicates", "evidence suggests", "studies confirm", "verified", "documented", "proven", "factually", "statistics show"],
        "indicators_negative": ["everyone knows", "obviously", "clearly", "always", "never", "all", "none", "definitely", "absolutely certain", "no doubt"],
        "weight": 1.0
    },
    "precision": {
        "name": "Precision",
        "color": "#9B59B6",
        "icon": "ğŸ¯",
        "description": "Is the statement specific and detailed enough?",
        "question": "Could you be more specific? Could you give more details?",
        "indicators_positive": ["exactly", "precisely", "approximately", "measured", "calculated", "percent", "ratio", "specifically", "in particular", "detailed"],
        "indicators_negative": ["a lot", "many", "few", "some", "often", "sometimes", "rarely", "big", "small", "good", "bad", "nice", "very"],
        "weight": 1.0
    },
    "relevance": {
        "name": "Relevance",
        "color": "#E67E22",
        "icon": "ğŸ”—",
        "description": "Does the statement relate to the issue at hand?",
        "question": "How does this relate to the problem? How does this help with the issue?",
        "indicators_positive": ["therefore", "consequently", "as a result", "this relates to", "connected to", "relevant because", "pertinent", "applicable", "bearing on", "in relation to"],
        "indicators_negative": ["by the way", "incidentally", "speaking of", "anyway", "besides", "also", "moreover", "furthermore", "in addition"],
        "weight": 1.0
    },
    "depth": {
        "name": "Depth",
        "color": "#E74C3C",
        "icon": "ğŸ“Š",
        "description": "Does the statement address the complexity of the issue?",
        "question": "What factors make this difficult? What are the complexities?",
        "indicators_positive": ["underlying", "fundamental", "root cause", "complexity", "nuanced", "multifaceted", "layers", "deeper", "systematic", "comprehensive", "thorough"],
        "indicators_negative": ["simple", "easy", "just", "only", "merely", "basic", "straightforward", "obvious solution"],
        "weight": 1.0
    },
    "breadth": {
        "name": "Breadth",
        "color": "#1ABC9C",
        "icon": "ğŸŒ",
        "description": "Does the statement consider other viewpoints?",
        "question": "Is there another way to look at this? What would this look like from another perspective?",
        "indicators_positive": ["alternatively", "on the other hand", "from another perspective", "considering also", "however", "conversely", "different view", "opposing argument", "some argue", "others believe"],
        "indicators_negative": ["the only way", "must be", "has to be", "no other", "single solution", "one answer"],
        "weight": 1.0
    },
    "logic": {
        "name": "Logic",
        "color": "#F1C40F",
        "icon": "âš™ï¸",
        "description": "Does the statement make sense and follow logically?",
        "question": "Does this follow from the evidence? Does this really make sense together?",
        "indicators_positive": ["because", "therefore", "thus", "hence", "consequently", "it follows that", "logically", "reasoning", "if then", "implies", "leads to"],
        "indicators_negative": ["but", "although", "despite", "regardless", "anyway", "still"],
        "weight": 1.0
    },
    "significance": {
        "name": "Significance",
        "color": "#8E44AD",
        "icon": "â­",
        "description": "Is this the most important issue to focus on?",
        "question": "Is this the most important problem to consider? Which of these facts is most important?",
        "indicators_positive": ["importantly", "significantly", "crucially", "essentially", "fundamentally", "key point", "primary", "central", "critical", "vital", "paramount"],
        "indicators_negative": ["trivial", "minor", "insignificant", "unimportant", "negligible"],
        "weight": 1.0
    },
    "fairness": {
        "name": "Fairness",
        "color": "#16A085",
        "icon": "âš–ï¸",
        "description": "Is the statement free from bias and self-interest?",
        "question": "Is my thinking justifiable? Am I considering others' viewpoints sympathetically?",
        "indicators_positive": ["objectively", "impartially", "fairly", "balanced", "unbiased", "neutral", "considering all", "without prejudice", "equitably", "justly"],
        "indicators_negative": ["obviously wrong", "stupid", "idiotic", "ridiculous", "absurd", "they always", "those people", "typical"],
        "weight": 1.0
    }
}

SCORE_LEVELS = {
    "excellent": {"min": 0.75, "color": "#2ECC71", "label": "Excellent", "icon": "ğŸŒŸ"},
    "good": {"min": 0.55, "color": "#3498DB", "label": "Good", "icon": "âœ…"},
    "adequate": {"min": 0.35, "color": "#F1C40F", "label": "Adequate", "icon": "âš ï¸"},
    "needs_work": {"min": 0.0, "color": "#E74C3C", "label": "Needs Improvement", "icon": "âŒ"}
}
# ============================================================
#Â  Â  Â  Â  Â  Â DATA EXTRACTION & PREPROCESSING
# ============================================================

def extract_text_from_file(file_path: Union[str, io.BytesIO], file_type: str) -> str:
Â  Â  text = ""
Â  Â  if file_type == 'pdf':
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  reader = pypdf.PdfReader(file_path)
Â  Â  Â  Â  Â  Â  for page in reader.pages:
Â  Â  Â  Â  Â  Â  Â  Â  text += page.extract_text() + "\n"
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  return f"ERROR_PDF_EXTRACTION: {e}"
Â  Â  elif file_type == 'docx':
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  document = docx.Document(file_path)
Â  Â  Â  Â  Â  Â  for paragraph in document.paragraphs:
Â  Â  Â  Â  Â  Â  Â  Â  text += paragraph.text + "\n"
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  return f"ERROR_DOCX_EXTRACTION: {e}"
Â  Â  elif file_type == 'txt':
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  if isinstance(file_path, str):
Â  Â  Â  Â  Â  Â  Â  Â  text = open(file_path, 'r', encoding='utf-8').read()
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  text = file_path.read().decode('utf-8')
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  return f"ERROR_TXT_EXTRACTION: {e}"
Â  Â  else:
Â  Â  Â  Â  return f"ERROR_UNSUPPORTED_TYPE: {file_type}"
Â  Â  return " ".join(text.split()).strip()

def preprocess_text(text: str) -> List[str]:
Â  Â  sentences = re.split(r'(?<=[.?!])\s+', text)
Â  Â  return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]

# ============================================================
#Â  Â  Â  Â  PAUL'S CRITICAL THINKING ANALYZER ENGINE
# ============================================================

class CriticalThinkingAnalyzer:
Â  Â  def __init__(self):
Â  Â  Â  Â  self.standards = PAUL_STANDARDS
Â  Â Â 
Â  Â  def analyze_standard(self, sentence: str, standard_key: str) -> Dict[str, Any]:
Â  Â  Â  Â  """Analyze a sentence against a specific standard"""
Â  Â  Â  Â  standard = self.standards[standard_key]
Â  Â  Â  Â  sentence_lower = sentence.lower()
Â  Â  Â  Â  words = sentence_lower.split()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Count positive and negative indicators
Â  Â  Â  Â  positive_count = 0
Â  Â  Â  Â  negative_count = 0
Â  Â  Â  Â  found_positive = []
Â  Â  Â  Â  found_negative = []
Â  Â  Â  Â Â 
Â  Â  Â  Â  for indicator in standard["indicators_positive"]:
Â  Â  Â  Â  Â  Â  if indicator.lower() in sentence_lower:
Â  Â  Â  Â  Â  Â  Â  Â  positive_count += 1
Â  Â  Â  Â  Â  Â  Â  Â  found_positive.append(indicator)
Â  Â  Â  Â Â 
Â  Â  Â  Â  for indicator in standard["indicators_negative"]:
Â  Â  Â  Â  Â  Â  if indicator.lower() in sentence_lower:
Â  Â  Â  Â  Â  Â  Â  Â  negative_count += 1
Â  Â  Â  Â  Â  Â  Â  Â  found_negative.append(indicator)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Calculate base score
Â  Â  Â  Â  base_score = 0.5Â  # Start neutral
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Adjust for positive indicators
Â  Â  Â  Â  base_score += min(positive_count * 0.15, 0.4)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Penalize for negative indicators
Â  Â  Â  Â  base_score -= min(negative_count * 0.12, 0.35)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Additional heuristics per standard
Â  Â  Â  Â  score_adjustment = self._apply_heuristics(sentence, standard_key, words)
Â  Â  Â  Â  base_score += score_adjustment
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Clamp score between 0 and 1
Â  Â  Â  Â  final_score = max(0.0, min(1.0, base_score))
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Determine level
Â  Â  Â  Â  level = self._get_score_level(final_score)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Generate feedback
Â  Â  Â  Â  feedback = self._generate_feedback(standard_key, final_score, found_positive, found_negative)
Â  Â  Â  Â Â 
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "standard": standard_key,
Â  Â  Â  Â  Â  Â  "standard_name": standard["name"],
Â  Â  Â  Â  Â  Â  "score": final_score,
Â  Â  Â  Â  Â  Â  "level": level,
Â  Â  Â  Â  Â  Â  "color": standard["color"],
Â  Â  Â  Â  Â  Â  "icon": standard["icon"],
Â  Â  Â  Â  Â  Â  "positive_indicators": found_positive,
Â  Â  Â  Â  Â  Â  "negative_indicators": found_negative,
Â  Â  Â  Â  Â  Â  "feedback": feedback,
Â  Â  Â  Â  Â  Â  "question": standard["question"]
Â  Â  Â  Â  }
Â  Â Â 
Â  Â  def _apply_heuristics(self, sentence: str, standard_key: str, words: List[str]) -> float:
Â  Â  Â  Â  """Apply additional heuristics based on sentence structure"""
Â  Â  Â  Â  adjustment = 0.0
Â  Â  Â  Â  sentence_lower = sentence.lower()
Â  Â  Â  Â Â 
Â  Â  Â  Â  if standard_key == "clarity":
Â  Â  Â  Â  Â  Â  # Longer sentences with proper structure tend to be clearer
Â  Â  Â  Â  Â  Â  if len(words) > 8 and len(words) < 30:
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.05
Â  Â  Â  Â  Â  Â  # Questions often seek clarity
Â  Â  Â  Â  Â  Â  if "?" in sentence:
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.05
Â  Â  Â  Â  Â  Â  # Very short sentences may lack clarity
Â  Â  Â  Â  Â  Â  if len(words) < 5:
Â  Â  Â  Â  Â  Â  Â  Â  adjustment -= 0.1
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  elif standard_key == "accuracy":
Â  Â  Â  Â  Â  Â  # Numbers and statistics suggest accuracy
Â  Â  Â  Â  Â  Â  if any(char.isdigit() for char in sentence):
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.1
Â  Â  Â  Â  Â  Â  # Quotes suggest citation
Â  Â  Â  Â  Â  Â  if '"' in sentence or "'" in sentence:
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.05
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  elif standard_key == "precision":
Â  Â  Â  Â  Â  Â  # Numbers indicate precision
Â  Â  Â  Â  Â  Â  digit_count = sum(1 for c in sentence if c.isdigit())
Â  Â  Â  Â  Â  Â  adjustment += min(digit_count * 0.03, 0.15)
Â  Â  Â  Â  Â  Â  # Percentages are precise
Â  Â  Â  Â  Â  Â  if "%" in sentence or "percent" in sentence_lower:
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.1
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  elif standard_key == "relevance":
Â  Â  Â  Â  Â  Â  # Connecting words show relevance
Â  Â  Â  Â  Â  Â  connectors = ["this", "that", "which", "these", "those"]
Â  Â  Â  Â  Â  Â  if any(c in words for c in connectors):
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.05
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  elif standard_key == "depth":
Â  Â  Â  Â  Â  Â  # Longer, complex sentences often show depth
Â  Â  Â  Â  Â  Â  if len(words) > 15:
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.08
Â  Â  Â  Â  Â  Â  # Multiple clauses suggest depth
Â  Â  Â  Â  Â  Â  if sentence.count(",") >= 2:
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.05
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  elif standard_key == "breadth":
Â  Â  Â  Â  Â  Â  # Comparative words show breadth
Â  Â  Â  Â  Â  Â  comparatives = ["while", "whereas", "compared", "contrast", "both", "either"]
Â  Â  Â  Â  Â  Â  if any(c in sentence_lower for c in comparatives):
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.1
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  elif standard_key == "logic":
Â  Â  Â  Â  Â  Â  # Causal language shows logic
Â  Â  Â  Â  Â  Â  causal = ["cause", "effect", "result", "lead", "due to", "since"]
Â  Â  Â  Â  Â  Â  if any(c in sentence_lower for c in causal):
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.1
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  elif standard_key == "significance":
Â  Â  Â  Â  Â  Â  # Emphasis words show significance awareness
Â  Â  Â  Â  Â  Â  emphasis = ["must", "need", "essential", "require", "necessary"]
Â  Â  Â  Â  Â  Â  if any(e in sentence_lower for e in emphasis):
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.08
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  elif standard_key == "fairness":
Â  Â  Â  Â  Â  Â  # First person plural suggests inclusivity
Â  Â  Â  Â  Â  Â  if "we" in words or "our" in words:
Â  Â  Â  Â  Â  Â  Â  Â  adjustment += 0.05
Â  Â  Â  Â  Â  Â  # Absolute language reduces fairness
Â  Â  Â  Â  Â  Â  absolutes = ["always", "never", "everyone", "no one", "all", "none"]
Â  Â  Â  Â  Â  Â  if any(a in words for a in absolutes):
Â  Â  Â  Â  Â  Â  Â  Â  adjustment -= 0.1
Â  Â  Â  Â Â 
Â  Â  Â  Â  return adjustment
Â  Â Â 
Â  Â  def _get_score_level(self, score: float) -> Dict[str, Any]:
Â  Â  Â  Â  """Determine the performance level based on score"""
Â  Â  Â  Â  for level_key, level_data in SCORE_LEVELS.items():
Â  Â  Â  Â  Â  Â  if score >= level_data["min"]:
Â  Â  Â  Â  Â  Â  Â  Â  return {"key": level_key, **level_data}
Â  Â  Â  Â  return {"key": "needs_work", **SCORE_LEVELS["needs_work"]}
Â  Â Â 
Â  Â  def _generate_feedback(self, standard_key: str, score: float,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  found_positive: List[str], found_negative: List[str]) -> str:
Â  Â  Â  Â  """Generate constructive feedback for the standard"""
Â  Â  Â  Â  standard = self.standards[standard_key]
Â  Â  Â  Â Â 
Â  Â  Â  Â  if score >= 0.75:
Â  Â  Â  Â  Â  Â  base = f"Excellent {standard['name'].lower()}! "
Â  Â  Â  Â  Â  Â  if found_positive:
Â  Â  Â  Â  Â  Â  Â  Â  base += f"Good use of: {', '.join(found_positive[:3])}."
Â  Â  Â  Â  elif score >= 0.55:
Â  Â  Â  Â  Â  Â  base = f"Good {standard['name'].lower()}. "
Â  Â  Â  Â  Â  Â  if found_negative:
Â  Â  Â  Â  Â  Â  Â  Â  base += f"Consider replacing: {', '.join(found_negative[:2])}."
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  base += f"Could strengthen with more specific language."
Â  Â  Â  Â  elif score >= 0.35:
Â  Â  Â  Â  Â  Â  base = f"Adequate {standard['name'].lower()}, but needs improvement. "
Â  Â  Â  Â  Â  Â  base += f"Ask yourself: {standard['question']}"
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  base = f"{standard['name']} needs significant improvement. "
Â  Â  Â  Â  Â  Â  if found_negative:
Â  Â  Â  Â  Â  Â  Â  Â  base += f"Avoid vague terms like: {', '.join(found_negative[:2])}. "
Â  Â  Â  Â  Â  Â  base += f"Consider: {standard['question']}"
Â  Â  Â  Â Â 
Â  Â  Â  Â  return base
Â  Â Â 
Â  Â  def analyze_sentence(self, sentence: str, index: int) -> Dict[str, Any]:
Â  Â  Â  Â  """Analyze a sentence against all Paul's Standards"""
Â  Â  Â  Â  results = {
Â  Â  Â  Â  Â  Â  "index": index,
Â  Â  Â  Â  Â  Â  "sentence": sentence,
Â  Â  Â  Â  Â  Â  "word_count": len(sentence.split()),
Â  Â  Â  Â  Â  Â  "standards": {},
Â  Â  Â  Â  Â  Â  "overall_score": 0.0,
Â  Â  Â  Â  Â  Â  "overall_level": None,
Â  Â  Â  Â  Â  Â  "strengths": [],
Â  Â  Â  Â  Â  Â  "weaknesses": [],
Â  Â  Â  Â  Â  Â  "recommendations": []
Â  Â  Â  Â  }
Â  Â  Â  Â Â 
Â  Â  Â  Â  total_score = 0.0
Â  Â  Â  Â  all_analyses = []
Â  Â  Â  Â Â 
Â  Â  Â  Â  for standard_key in self.standards:
Â  Â  Â  Â  Â  Â  analysis = self.analyze_standard(sentence, standard_key)
Â  Â  Â  Â  Â  Â  results["standards"][standard_key] = analysis
Â  Â  Â  Â  Â  Â  total_score += analysis["score"]
Â  Â  Â  Â  Â  Â  all_analyses.append((standard_key, analysis["score"]))
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Calculate overall score
Â  Â  Â  Â  results["overall_score"] = total_score / len(self.standards)
Â  Â  Â  Â  results["overall_level"] = self._get_score_level(results["overall_score"])
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Identify strengths and weaknesses
Â  Â  Â  Â  sorted_analyses = sorted(all_analyses, key=lambda x: x[1], reverse=True)
Â  Â  Â  Â  results["strengths"] = [self.standards[s[0]]["name"] for s in sorted_analyses[:3] if s[1] >= 0.55]
Â  Â  Â  Â  results["weaknesses"] = [self.standards[s[0]]["name"] for s in sorted_analyses[-3:] if s[1] < 0.55]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Generate recommendations
Â  Â  Â  Â  for standard_key, score in sorted_analyses[-2:]:
Â  Â  Â  Â  Â  Â  if score < 0.55:
Â  Â  Â  Â  Â  Â  Â  Â  results["recommendations"].append(self.standards[standard_key]["question"])
Â  Â  Â  Â Â 
Â  Â  Â  Â  return results
Â  Â Â 
Â  Â  def analyze_document(self, sentences: List[str], doc_name: str, doc_id: str) -> Dict[str, Any]:
Â  Â  Â  Â  """Analyze an entire document"""
Â  Â  Â  Â  sentence_results = []
Â  Â  Â  Â  standard_totals = {k: 0.0 for k in self.standards}
Â  Â  Â  Â Â 
Â  Â  Â  Â  for i, sentence in enumerate(sentences):
Â  Â  Â  Â  Â  Â  result = self.analyze_sentence(sentence, i + 1)
Â  Â  Â  Â  Â  Â  result["document_name"] = doc_name
Â  Â  Â  Â  Â  Â  result["document_id"] = doc_id
Â  Â  Â  Â  Â  Â  sentence_results.append(result)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  for standard_key in self.standards:
Â  Â  Â  Â  Â  Â  Â  Â  standard_totals[standard_key] += result["standards"][standard_key]["score"]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Calculate document-level statistics
Â  Â  Â  Â  num_sentences = len(sentences)
Â  Â  Â  Â  standard_averages = {k: v / num_sentences for k, v in standard_totals.items()}
Â  Â  Â  Â  overall_avg = sum(standard_averages.values()) / len(standard_averages)
Â  Â  Â  Â Â 
Â  Â  Â  Â  return {
Â  Â  Â  Â  Â  Â  "document_name": doc_name,
Â  Â  Â  Â  Â  Â  "document_id": doc_id,
Â  Â  Â  Â  Â  Â  "total_sentences": num_sentences,
Â  Â  Â  Â  Â  Â  "sentence_results": sentence_results,
Â  Â  Â  Â  Â  Â  "standard_averages": standard_averages,
Â  Â  Â  Â  Â  Â  "overall_average": overall_avg,
Â  Â  Â  Â  Â  Â  "overall_level": self._get_score_level(overall_avg)
Â  Â  Â  Â  }

# ============================================================
#Â  Â  Â  Â  Â  Â  Â  VISUALIZATION FUNCTIONS
# ============================================================

def create_standard_radar(averages: Dict[str, float], title: str):
Â  Â  """Create radar chart for standards overview"""
Â  Â  categories = [PAUL_STANDARDS[k]["name"] for k in averages.keys()]
Â  Â  values = list(averages.values())
Â  Â Â 
Â  Â  fig = go.Figure()
Â  Â  fig.add_trace(go.Scatterpolar(
Â  Â  Â  Â  r=values + [values[0]],
Â  Â  Â  Â  theta=categories + [categories[0]],
Â  Â  Â  Â  fill='toself',
Â  Â  Â  Â  fillcolor='rgba(102, 126, 234, 0.3)',
Â  Â  Â  Â  line=dict(color='#667eea', width=2),
Â  Â  Â  Â  name='Score'
Â  Â  ))
Â  Â Â 
Â  Â  fig.update_layout(
Â  Â  Â  Â  polar=dict(
Â  Â  Â  Â  Â  Â  radialaxis=dict(visible=True, range=[0, 1], tickvals=[0.25, 0.5, 0.75, 1.0]),
Â  Â  Â  Â  Â  Â  angularaxis=dict(tickfont=dict(size=11))
Â  Â  Â  Â  ),
Â  Â  Â  Â  title=title,
Â  Â  Â  Â  height=450,
Â  Â  Â  Â  template="plotly_dark"
Â  Â  )
Â  Â  return fig

def create_standards_bar_chart(averages: Dict[str, float], title: str):
Â  Â  """Create bar chart with Paul's Standards colors"""
Â  Â  data = []
Â  Â  for key, score in averages.items():
Â  Â  Â  Â  data.append({
Â  Â  Â  Â  Â  Â  "Standard": PAUL_STANDARDS[key]["name"],
Â  Â  Â  Â  Â  Â  "Score": score,
Â  Â  Â  Â  Â  Â  "Color": PAUL_STANDARDS[key]["color"],
Â  Â  Â  Â  Â  Â  "Icon": PAUL_STANDARDS[key]["icon"]
Â  Â  Â  Â  })
Â  Â Â 
Â  Â  df = pd.DataFrame(data)
Â  Â  df = df.sort_values("Score", ascending=True)
Â  Â Â 
Â  Â  fig = px.bar(df, x="Score", y="Standard", orientation='h',
Â  Â  Â  Â  Â  Â  Â  Â  Â color="Standard", color_discrete_map={
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â PAUL_STANDARDS[k]["name"]: PAUL_STANDARDS[k]["color"]Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â for k in PAUL_STANDARDS
Â  Â  Â  Â  Â  Â  Â  Â  Â })
Â  Â Â 
Â  Â  fig.update_layout(
Â  Â  Â  Â  title=title,
Â  Â  Â  Â  xaxis=dict(range=[0, 1], title="Score"),
Â  Â  Â  Â  yaxis=dict(title=""),
Â  Â  Â  Â  height=400,
Â  Â  Â  Â  template="plotly_dark",
Â  Â  Â  Â  showlegend=False
Â  Â  )
Â  Â  return fig

def create_sentence_heatmap(sentence_results: List[Dict], title: str):
Â  Â  """Create heatmap of all sentences vs standards"""
Â  Â  if not sentence_results:
Â  Â  Â  Â  return go.Figure()
Â  Â Â 
Â  Â  # Build matrix
Â  Â  standards_list = list(PAUL_STANDARDS.keys())
Â  Â  matrix = []
Â  Â  labels = []
Â  Â Â 
Â  Â  for result in sentence_results[:30]:Â  # Limit for readability
Â  Â  Â  Â  row = [result["standards"][s]["score"] for s in standards_list]
Â  Â  Â  Â  matrix.append(row)
Â  Â  Â  Â  labels.append(f"S{result['index']}")
Â  Â Â 
Â  Â  fig = px.imshow(
Â  Â  Â  Â  matrix,
Â  Â  Â  Â  x=[PAUL_STANDARDS[s]["name"] for s in standards_list],
Â  Â  Â  Â  y=labels,
Â  Â  Â  Â  color_continuous_scale="RdYlGn",
Â  Â  Â  Â  aspect="auto",
Â  Â  Â  Â  title=title
Â  Â  )
Â  Â Â 
Â  Â  fig.update_layout(
Â  Â  Â  Â  height=max(400, len(labels) * 25),
Â  Â  Â  Â  template="plotly_dark",
Â  Â  Â  Â  xaxis=dict(tickangle=45)
Â  Â  )
Â  Â  return fig

def create_score_distribution(sentence_results: List[Dict], title: str):
Â  Â  """Create distribution of overall scores"""
Â  Â  scores = [r["overall_score"] for r in sentence_results]
Â  Â Â 
Â  Â  fig = go.Figure()
Â  Â  fig.add_trace(go.Histogram(
Â  Â  Â  Â  x=scores,
Â  Â  Â  Â  nbinsx=20,
Â  Â  Â  Â  marker_color='#667eea',
Â  Â  Â  Â  opacity=0.8
Â  Â  ))
Â  Â Â 
Â  Â  # Add threshold lines
Â  Â  for level_key, level_data in SCORE_LEVELS.items():
Â  Â  Â  Â  if level_data["min"] > 0:
Â  Â  Â  Â  Â  Â  fig.add_vline(x=level_data["min"], line_dash="dash",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â line_color=level_data["color"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â annotation_text=level_data["label"])
Â  Â Â 
Â  Â  fig.update_layout(
Â  Â  Â  Â  title=title,
Â  Â  Â  Â  xaxis_title="Overall Score",
Â  Â  Â  Â  yaxis_title="Number of Sentences",
Â  Â  Â  Â  height=350,
Â  Â  Â  Â  template="plotly_dark"
Â  Â  )
Â  Â  return fig

def create_progress_chart(sentence_results: List[Dict], title: str):
Â  Â  """Show how scores progress through the document"""
Â  Â  if not sentence_results:
Â  Â  Â  Â  return go.Figure()
Â  Â Â 
Â  Â  fig = go.Figure()
Â  Â Â 
Â  Â  # Overall score line
Â  Â  fig.add_trace(go.Scatter(
Â  Â  Â  Â  x=[r["index"] for r in sentence_results],
Â  Â  Â  Â  y=[r["overall_score"] for r in sentence_results],
Â  Â  Â  Â  mode='lines+markers',
Â  Â  Â  Â  name='Overall Score',
Â  Â  Â  Â  line=dict(color='#667eea', width=3)
Â  Â  ))
Â  Â Â 
Â  Â  # Add trend line
Â  Â  x = np.array([r["index"] for r in sentence_results])
Â  Â  y = np.array([r["overall_score"] for r in sentence_results])
Â  Â  z = np.polyfit(x, y, 1)
Â  Â  p = np.poly1d(z)
Â  Â Â 
Â  Â  fig.add_trace(go.Scatter(
Â  Â  Â  Â  x=x,
Â  Â  Â  Â  y=p(x),
Â  Â  Â  Â  mode='lines',
Â  Â  Â  Â  name='Trend',
Â  Â  Â  Â  line=dict(color='#E74C3C', width=2, dash='dash')
Â  Â  ))
Â  Â Â 
Â  Â  fig.update_layout(
Â  Â  Â  Â  title=title,
Â  Â  Â  Â  xaxis_title="Sentence Number",
Â  Â  Â  Â  yaxis_title="Score",
Â  Â  Â  Â  yaxis=dict(range=[0, 1]),
Â  Â  Â  Â  height=350,
Â  Â  Â  Â  template="plotly_dark"
Â  Â  )
Â  Â  return fig

# ============================================================
#Â  Â  Â  Â  Â  Â REPORT GENERATION FUNCTIONS
# ============================================================

def generate_sentence_report_html(result: Dict) -> str:
Â  Â  """Generate HTML report for a single sentence"""
Â  Â  html = f"""
Â  Â  <div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);Â 
Â  Â  Â  Â  Â  Â  Â  Â  padding: 20px; border-radius: 15px; margin: 15px 0;Â 
Â  Â  Â  Â  Â  Â  Â  Â  border-left: 5px solid {result['overall_level']['color']};">
Â  Â  Â  Â  <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 15px;">
Â  Â  Â  Â  Â  Â  <h4 style="margin: 0; color: white;">ğŸ“ Sentence {result['index']}</h4>
Â  Â  Â  Â  Â  Â  <span style="background: {result['overall_level']['color']}; padding: 5px 15px;Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  border-radius: 20px; color: white; font-weight: bold;">
Â  Â  Â  Â  Â  Â  Â  Â  {result['overall_level']['icon']} {result['overall_level']['label']} ({result['overall_score']:.0%})
Â  Â  Â  Â  Â  Â  </span>
Â  Â  Â  Â  </div>
Â  Â  Â  Â  <p style="color: #ccc; font-style: italic; margin-bottom: 20px;Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  padding: 10px; background: rgba(255,255,255,0.05); border-radius: 8px;">
Â  Â  Â  Â  Â  Â  "{result['sentence']}"
Â  Â  Â  Â  </p>
Â  Â  Â  Â  <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px;">
Â  Â  """
Â  Â Â 
Â  Â  for standard_key, analysis in result["standards"].items():
Â  Â  Â  Â  level_color = analysis["level"]["color"]
Â  Â  Â  Â  html += f"""
Â  Â  Â  Â  Â  Â  <div style="background: rgba(255,255,255,0.05); padding: 12px; border-radius: 10px;
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  border-top: 3px solid {analysis['color']};">
Â  Â  Â  Â  Â  Â  Â  Â  <div style="display: flex; justify-content: space-between; align-items: center;">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <span style="color: {analysis['color']}; font-weight: bold;">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {analysis['icon']} {analysis['standard_name']}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </span>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <span style="background: {level_color}; padding: 2px 8px; border-radius: 10px;Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  font-size: 12px; color: white;">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {analysis['score']:.0%}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </span>
Â  Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  Â  Â  <p style="color: #999; font-size: 11px; margin: 8px 0 0 0;">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {analysis['feedback'][:100]}{'...' if len(analysis['feedback']) > 100 else ''}
Â  Â  Â  Â  Â  Â  Â  Â  </p>
Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  """
Â  Â Â 
Â  Â  html += "</div>"
Â  Â Â 
Â  Â  # Strengths and Weaknesses
Â  Â  if result["strengths"] or result["weaknesses"]:
Â  Â  Â  Â  html += '<div style="display: flex; gap: 20px; margin-top: 15px;">'
Â  Â  Â  Â  if result["strengths"]:
Â  Â  Â  Â  Â  Â  html += f"""
Â  Â  Â  Â  Â  Â  Â  Â  <div style="flex: 1; background: rgba(46, 204, 113, 0.1); padding: 10px; border-radius: 8px;">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <strong style="color: #2ECC71;">ğŸ’ª Strengths:</strong>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <span style="color: #ccc;"> {', '.join(result['strengths'])}</span>
Â  Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  if result["weaknesses"]:
Â  Â  Â  Â  Â  Â  html += f"""
Â  Â  Â  Â  Â  Â  Â  Â  <div style="flex: 1; background: rgba(231, 76, 60, 0.1); padding: 10px; border-radius: 8px;">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <strong style="color: #E74C3C;">ğŸ¯ Areas to Improve:</strong>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <span style="color: #ccc;"> {', '.join(result['weaknesses'])}</span>
Â  Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  html += "</div>"
Â  Â Â 
Â  Â  html += "</div>"
Â  Â  return html

def generate_full_report_html(doc_result: Dict) -> str:
Â  Â  """Generate complete HTML report for download"""
Â  Â  html = f"""
Â  Â  <!DOCTYPE html>
Â  Â  <html>
Â  Â  <head>
Â  Â  Â  Â  <meta charset="UTF-8">
Â  Â  Â  Â  <title>Critical Thinking Analysis Report - {doc_result['document_name']}</title>
Â  Â  Â  Â  <style>
Â  Â  Â  Â  Â  Â  body {{ font-family: 'Segoe UI', Arial, sans-serif; background: #0f0f23; color: #fff; padding: 40px; }}
Â  Â  Â  Â  Â  Â  .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 15px; text-align: center; margin-bottom: 30px; }}
Â  Â  Â  Â  Â  Â  .metric-grid {{ display: grid; grid-template-columns: repeat(4, 1fr); gap: 15px; margin-bottom: 30px; }}
Â  Â  Â  Â  Â  Â  .metric {{ background: #1a1a2e; padding: 20px; border-radius: 10px; text-align: center; }}
Â  Â  Â  Â  Â  Â  .metric-value {{ font-size: 2em; font-weight: bold; color: #667eea; }}
Â  Â  Â  Â  Â  Â  .metric-label {{ color: #888; font-size: 0.9em; }}
Â  Â  Â  Â  Â  Â  .standard-card {{ background: #1a1a2e; padding: 15px; border-radius: 10px; margin: 10px 0; }}
Â  Â  Â  Â  Â  Â  .sentence-report {{ background: #16213e; padding: 20px; border-radius: 15px; margin: 20px 0; }}
Â  Â  Â  Â  Â  Â  .score-badge {{ display: inline-block; padding: 5px 15px; border-radius: 20px; font-weight: bold; }}
Â  Â  Â  Â  Â  Â  table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
Â  Â  Â  Â  Â  Â  th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #333; }}
Â  Â  Â  Â  Â  Â  th {{ background: #667eea; }}
Â  Â  Â  Â  </style>
Â  Â  </head>
Â  Â  <body>
Â  Â  Â  Â  <div class="header">
Â  Â  Â  Â  Â  Â  <h1>ğŸ§  Critical Thinking Analysis Report</h1>
Â  Â  Â  Â  Â  Â  <p>Based on Paul's Universal Intellectual Standards</p>
Â  Â  Â  Â  Â  Â  <p>Document: {doc_result['document_name']} | ID: {doc_result['document_id']}</p>
Â  Â  Â  Â  Â  Â  <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}</p>
Â  Â  Â  Â  </div>
Â  Â  Â  Â Â 
Â  Â  Â  Â  <div class="metric-grid">
Â  Â  Â  Â  Â  Â  <div class="metric">
Â  Â  Â  Â  Â  Â  Â  Â  <div class="metric-value">{doc_result['overall_level']['icon']}</div>
Â  Â  Â  Â  Â  Â  Â  Â  <div class="metric-label">{doc_result['overall_level']['label']}</div>
Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  <div class="metric">
Â  Â  Â  Â  Â  Â  Â  Â  <div class="metric-value">{max(doc_result['standard_averages'], key=doc_result['standard_averages'].get).title()}</div>
Â  Â  Â  Â  Â  Â  Â  Â  <div class="metric-label">Strongest Area</div>
Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  </div>
Â  Â  Â  Â Â 
Â  Â  Â  Â  <h2>ğŸ“Š Standards Overview</h2>
Â  Â  Â  Â  <table>
Â  Â  Â  Â  Â  Â  <tr>
Â  Â  Â  Â  Â  Â  Â  Â  <th>Standard</th>
Â  Â  Â  Â  Â  Â  Â  Â  <th>Score</th>
Â  Â  Â  Â  Â  Â  Â  Â  <th>Level</th>
Â  Â  Â  Â  Â  Â  Â  Â  <th>Description</th>
Â  Â  Â  Â  Â  Â  </tr>
Â  Â  """
Â  Â Â 
Â  Â  for key, score in doc_result['standard_averages'].items():
Â  Â  Â  Â  std = PAUL_STANDARDS[key]
Â  Â  Â  Â  level = "Excellent" if score >= 0.75 else "Good" if score >= 0.55 else "Adequate" if score >= 0.35 else "Needs Work"
Â  Â  Â  Â  level_color = "#2ECC71" if score >= 0.75 else "#3498DB" if score >= 0.55 else "#F1C40F" if score >= 0.35 else "#E74C3C"
Â  Â  Â  Â  html += f"""
Â  Â  Â  Â  Â  Â  <tr>
Â  Â  Â  Â  Â  Â  Â  Â  <td style="color: {std['color']}; font-weight: bold;">{std['icon']} {std['name']}</td>
Â  Â  Â  Â  Â  Â  Â  Â  <td>{score:.0%}</td>
Â  Â  Â  Â  Â  Â  Â  Â  <td><span style="background: {level_color}; padding: 3px 10px; border-radius: 10px;">{level}</span></td>
Â  Â  Â  Â  Â  Â  Â  Â  <td style="color: #888;">{std['description']}</td>
Â  Â  Â  Â  Â  Â  </tr>
Â  Â  Â  Â  """
Â  Â Â 
Â  Â  html += """
Â  Â  Â  Â  </table>
Â  Â  Â  Â Â 
Â  Â  Â  Â  <h2>ğŸ“ Detailed Sentence Analysis</h2>
Â  Â  """
Â  Â Â 
Â  Â  for result in doc_result['sentence_results']:
Â  Â  Â  Â  html += f"""
Â  Â  Â  Â  <div class="sentence-report" style="border-left: 4px solid {result['overall_level']['color']};">
Â  Â  Â  Â  Â  Â  <div style="display: flex; justify-content: space-between; margin-bottom: 10px;">
Â  Â  Â  Â  Â  Â  Â  Â  <strong>Sentence {result['index']}</strong>
Â  Â  Â  Â  Â  Â  Â  Â  <span class="score-badge" style="background: {result['overall_level']['color']};">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {result['overall_level']['icon']} {result['overall_score']:.0%}
Â  Â  Â  Â  Â  Â  Â  Â  </span>
Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  <p style="color: #aaa; font-style: italic;">"{result['sentence']}"</p>
Â  Â  Â  Â  Â  Â  <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 8px; margin-top: 15px;">
Â  Â  Â  Â  """
Â  Â  Â  Â Â 
Â  Â  Â  Â  for std_key, analysis in result['standards'].items():
Â  Â  Â  Â  Â  Â  html += f"""
Â  Â  Â  Â  Â  Â  Â  Â  <div style="background: rgba(255,255,255,0.05); padding: 8px; border-radius: 8px; border-top: 2px solid {analysis['color']};">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <span style="color: {analysis['color']};">{analysis['icon']} {analysis['standard_name']}: </span>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <span style="color: {analysis['level']['color']};">{analysis['score']:.0%}</span>
Â  Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â Â 
Â  Â  Â  Â  html += """
Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  </div>
Â  Â  Â  Â  """
Â  Â Â 
Â  Â  html += """
Â  Â  </body>
Â  Â  </html>
Â  Â  """
Â  Â  return html

# ============================================================
#Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  MAIN STREAMLIT APP
# ============================================================

def main():
Â  Â  st.set_page_config(layout="wide", page_title="Paul's Critical Thinking Analyzer", page_icon="ğŸ§ ")
Â  Â Â 
Â  Â  # Custom CSS
Â  Â  st.markdown("""
Â  Â  <style>
Â  Â  .main-header {
Â  Â  Â  Â  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
Â  Â  Â  Â  padding: 2rem;
Â  Â  Â  Â  border-radius: 15px;
Â  Â  Â  Â  margin-bottom: 2rem;
Â  Â  Â  Â  text-align: center;
Â  Â  }
Â  Â  .main-header h1 { color: white; margin: 0; }
Â  Â  .main-header p { color: rgba(255,255,255,0.8); margin: 10px 0 0 0; }
Â  Â  .standard-badge {
Â  Â  Â  Â  display: inline-block;
Â  Â  Â  Â  padding: 8px 16px;
Â  Â  Â  Â  border-radius: 20px;
Â  Â  Â  Â  margin: 5px;
Â  Â  Â  Â  font-weight: bold;
Â  Â  }
Â  Â  .metric-card {
Â  Â  Â  Â  background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
Â  Â  Â  Â  padding: 1.5rem;
Â  Â  Â  Â  border-radius: 12px;
Â  Â  Â  Â  text-align: center;
Â  Â  }
    .metric-value-large {
        font-size: 3em;
        font-weight: bold;
        color: #667eea;
        margin-bottom: 0.2em;
    }
    .metric-label-small {
        color: #888;
        font-size: 0.9em;
    }
    .sidebar-header {
        color: white;
        text-align: center;
        margin-bottom: 1.5rem;
    }
    .stCodeBlock {
        background-color: #1a1a2e !important;
        border: 1px solid #333;
    }
    </style>
    """, unsafe_allow_html=True)

    st.markdown('<div class="main-header"><h1>ğŸ§  Paul\'s Critical Thinking Analyzer</h1><p>Assess text against Paul\'s Universal Intellectual Standards (Clarity, Accuracy, Precision, etc.)</p></div>', unsafe_allow_html=True)

    # --- Sidebar for Input ---
    with st.sidebar:
        st.markdown('<div class="sidebar-header"><h2>Input Text</h2></div>', unsafe_allow_html=True)

        input_method = st.radio("Choose Input Method", ["Text Input", "File Upload"], index=0)

        uploaded_file = None
        user_text = ""
        doc_name = "Untitled Document"

        if input_method == "File Upload":
            uploaded_file = st.file_uploader("Upload Document (.txt, .pdf, .docx)", type=['txt', 'pdf', 'docx'])
            if uploaded_file:
                doc_name = uploaded_file.name
                file_type = uploaded_file.name.split('.')[-1].lower()
                text_result = extract_text_from_file(uploaded_file, file_type)
                
                if text_result.startswith("ERROR_"):
                    st.error(f"File extraction failed: {text_result}")
                else:
                    user_text = text_result
        else:
            user_text = st.text_area("Paste Text for Analysis", height=300, 
                                     value="The current economic policy is obviously flawed. It should be changed immediately because everyone knows a different system will clearly yield better results, but all the politicians are too corrupt to understand the simple solution.")
            doc_name = "User Input Text"
        
        # Add a section to display the raw text for verification
        st.subheader("Raw Text Preview")
        st.code(user_text[:500] + ('...' if len(user_text) > 500 else ''), language='text')

    # --- Main Content Area ---
    if user_text:
        # Preprocess and Analyze
        try:
            sentences = preprocess_text(user_text)
            if not sentences:
                st.warning("The extracted text is too short or could not be properly segmented into sentences (min 10 characters).")
                st.stop()

            analyzer = CriticalThinkingAnalyzer()
            doc_id = str(hash(user_text)) # Simple deterministic ID
            doc_result = analyzer.analyze_document(sentences, doc_name, doc_id)

            st.success(f"Analysis Complete: {doc_result['total_sentences']} sentences processed.")
            
            # 1. Overall Metrics
            st.header("ğŸ“Š Document Overview")
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value-large" style="color: {doc_result['overall_level']['color']};">{doc_result['overall_level']['icon']}</div>
                    <div class="metric-label-small">Overall Level: <strong>{doc_result['overall_level']['label']}</strong></div>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value-large">{doc_result['overall_average']:.1%}</div>
                    <div class="metric-label-small">Average Critical Thinking Score</div>
                </div>
                """, unsafe_allow_html=True)

            strongest = max(doc_result['standard_averages'], key=doc_result['standard_averages'].get)
            weakest = min(doc_result['standard_averages'], key=doc_result['standard_averages'].get)

            with col3:
                 st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value-large" style="color: {PAUL_STANDARDS[strongest]['color']};">{PAUL_STANDARDS[strongest]['icon']}</div>
                    <div class="metric-label-small">Strongest Standard: <strong>{PAUL_STANDARDS[strongest]['name']}</strong></div>
                </div>
                """, unsafe_allow_html=True)

            with col4:
                st.markdown(f"""
                <div class="metric-card">
                    <div class="metric-value-large" style="color: {PAUL_STANDARDS[weakest]['color']};">{PAUL_STANDARDS[weakest]['icon']}</div>
                    <div class="metric-label-small">Weakest Standard: <strong>{PAUL_STANDARDS[weakest]['name']}</strong></div>
                </div>
                """, unsafe_allow_html=True)

            st.markdown("---")

            # 2. Visualization Charts
            st.header("ğŸ“ˆ Visualization of Standards")
            chart_col1, chart_col2 = st.columns(2)

            with chart_col1:
                st.plotly_chart(create_standard_radar(doc_result['standard_averages'], "Standards Radar Chart"), use_container_width=True)

            with chart_col2:
                st.plotly_chart(create_standards_bar_chart(doc_result['standard_averages'], "Average Score by Standard"), use_container_width=True)

            st.plotly_chart(create_progress_chart(doc_result['sentence_results'], "Critical Thinking Score Progression"), use_container_width=True)
            
            if len(doc_result['sentence_results']) > 1:
                st.plotly_chart(create_sentence_heatmap(doc_result['sentence_results'], "Sentence-by-Sentence Score Heatmap (First 30)"), use_container_width=True)

            st.markdown("---")

            # 3. Detailed Sentence Breakdown
            st.header("ğŸ“ Detailed Sentence Breakdown")
            
            # Dropdown/Selector for Sentence
            sentence_options = {r['index']: r['sentence'][:70] + '...' for r in doc_result['sentence_results']}
            selected_index = st.selectbox("Select a Sentence to View Details", options=list(sentence_options.keys()), format_func=lambda x: f"Sentence {x}: {sentence_options[x]}")

            selected_result = next((r for r in doc_result['sentence_results'] if r['index'] == selected_index), None)

            if selected_result:
                st.markdown(generate_sentence_report_html(selected_result), unsafe_allow_html=True)

            st.markdown("---")

            # 4. Download Report
            st.header("â¬‡ï¸ Download Full Report")
            full_html_report = generate_full_report_html(doc_result)
            
            st.download_button(
                label="Download Full HTML Report",
                data=full_html_report,
                file_name=f"{doc_name.replace(' ', '_')}_critical_thinking_report.html",
                mime="text/html"
            )


        except Exception as e:
            st.error(f"An error occurred during analysis: {e}")
            st.exception(e)

    else:
        st.info("Paste or upload a document in the sidebar to begin the Critical Thinking Analysis.")

if __name__ == '__main__':
Â  Â  main()
